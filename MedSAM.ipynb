{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095d526c-41d1-4416-9291-26276ade2d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Torch: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# Minimal deps for MedSAM-based zero-shot labeling\n",
    "%pip install -q --no-deps \"git+https://github.com/facebookresearch/segment-anything.git\" scikit-image opencv-python seaborn huggingface_hub\n",
    "\n",
    "import torch\n",
    "print(\"Torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b121168-c857-48d3-ac47-cb08a4327e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm configured for text output. Type: <class 'type'>\n"
     ]
    }
   ],
   "source": [
    "# Run this ONCE per kernel, before other tqdm imports\n",
    "import os, sys\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"   # tell tqdm to not use notebook widgets\n",
    "os.environ[\"TQDM_DISABLE\"] = \"\"     # ensure tqdm is enabled (not disabled)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional: clear any previous widget placeholders\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    clear_output(wait=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"tqdm configured for text output. Type:\", type(tqdm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd5966a-5b8b-4e08-b464-9e8312b2cca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: /home/philipdt/IKT-project/data/EndoscopicBladderTissue\n",
      "Output labels: /home/philipdt/IKT-project/auto_labels_medsam\n",
      "Class set: extended {0: 'background', 1: 'vessel', 2: 'mucosa', 3: 'tumor', 4: 'instrument', 5: 'specular'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CFG:\n",
    "    # Input images: directly under each class folder\n",
    "    ORIG_ROOT = \"/home/philipdt/IKT-project/data/EndoscopicBladderTissue\"\n",
    "    CLASSES   = [\"HGC\", \"LGC\", \"NST\", \"NTL\"]\n",
    "\n",
    "    # Choose class set: \"basic\" or \"extended\"\n",
    "    CLASS_SET = \"extended\"\n",
    "\n",
    "    CLASS_SETS = {\n",
    "        \"basic\":    {0:\"background\", 1:\"vessel\", 2:\"mucosa\"},\n",
    "        \"extended\": {0:\"background\", 1:\"vessel\", 2:\"mucosa\", 3:\"tumor\", 4:\"instrument\", 5:\"specular\"}\n",
    "    }\n",
    "    CLASS_COLORS = {\n",
    "        \"basic\":    {0:(0,0,0), 1:(0,255,255), 2:(255,165,0)},\n",
    "        \"extended\": {0:(0,0,0), 1:(0,255,255), 2:(255,165,0), 3:(255,0,0), 4:(128,128,128), 5:(255,255,255)}\n",
    "    }\n",
    "\n",
    "    # MedSAM settings (keep as is)\n",
    "    SAM_TYPE = \"vit_b\"\n",
    "    MEDSAM_CKPT = \"/home/philipdt/IKT-project/weights/medsam_vit_b.pth\"\n",
    "    AUTO_DOWNLOAD = True\n",
    "    OUTPUT_LABEL_ROOT = \"/home/philipdt/IKT-project/auto_labels_medsam\"\n",
    "\n",
    "    # SAM generator knobs (keep as is)\n",
    "    POINTS_PER_SIDE = 24\n",
    "    PRED_IOU_THRESH = 0.88\n",
    "    STABILITY_THRESH = 0.90\n",
    "    CROP_N_LAYERS = 1\n",
    "    CROP_DOWNSCALE = 2\n",
    "    MIN_MASK_REGION_AREA = 256\n",
    "    COVERAGE_LIMIT = 0.70\n",
    "\n",
    "    # THESE ARE THE ONES TO CHANGE - make them more permissive:\n",
    "    VESSEL_SIGMA_RANGE = (1.0, 3.0)   # keep same\n",
    "    VESSEL_PCT = 95.0                 # CHANGED from 98.0\n",
    "    SPECULAR_V_MIN = 200              # CHANGED from 230\n",
    "    SPECULAR_S_MAX = 50               # CHANGED from 40\n",
    "    INSTR_EDGE_PCT = 0.08             # CHANGED from 0.12\n",
    "    TUMOR_RED_RATIO = 1.15            # CHANGED from 1.25\n",
    "    MIN_REGION_AREA = 256             # keep same\n",
    "\n",
    "# Validate paths\n",
    "if not Path(CFG.ORIG_ROOT).is_dir():\n",
    "    raise SystemExit(f\"ORIG_ROOT not found: {CFG.ORIG_ROOT}\")\n",
    "Path(CFG.OUTPUT_LABEL_ROOT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Input:\", CFG.ORIG_ROOT)\n",
    "print(\"Output labels:\", CFG.OUTPUT_LABEL_ROOT)\n",
    "print(\"Class set:\", CFG.CLASS_SET, CFG.CLASS_SETS[CFG.CLASS_SET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271c802e-c865-441b-9de4-b16047407adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities (force console tqdm to avoid \"Loading widget...\" hang)\n",
    "import os, numpy as np, cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from skimage.filters import frangi\n",
    "\n",
    "# Force non-widget tqdm\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"\n",
    "try:\n",
    "    from tqdm.std import tqdm  # text mode\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "IMG_EXTS = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\",\".PNG\",\".JPG\"}\n",
    "\n",
    "def list_images(d: Path) -> List[Path]:\n",
    "    if not d.is_dir(): return []\n",
    "    return sorted([p for p in d.iterdir() if p.is_file() and p.suffix in IMG_EXTS])\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def colorize_mask(mask: np.ndarray, colors: Dict[int, Tuple[int,int,int]]) -> np.ndarray:\n",
    "    out = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for k, c in colors.items():\n",
    "        out[mask == k] = c\n",
    "    return out\n",
    "\n",
    "def vesselness(gray_u8: np.ndarray, sig_range=(1.0,3.0)) -> np.ndarray:\n",
    "    g = gray_u8.astype(np.float32) / 255.0\n",
    "    v = frangi(g, sigmas=np.linspace(sig_range[0], sig_range[1], 4), black_ridges=False)\n",
    "    v = np.nan_to_num(v)\n",
    "    v = (v - v.min()) / (v.max() - v.min() + 1e-6)\n",
    "    return v\n",
    "\n",
    "def specular_mask(bgr: np.ndarray, v_min=230, s_max=40) -> np.ndarray:\n",
    "    hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
    "    h,s,v = cv2.split(hsv)\n",
    "    return ((v >= v_min) & (s <= s_max)).astype(np.uint8)\n",
    "\n",
    "def instrument_mask(bgr: np.ndarray, edge_pct=0.12) -> np.ndarray:\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 80, 160)\n",
    "    return edges, edges.mean() >= edge_pct*255\n",
    "\n",
    "def tumor_likelihood(bgr: np.ndarray, red_ratio=1.25) -> np.ndarray:\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB).astype(np.float32) + 1e-6\n",
    "    R, G, B = rgb[...,0], rgb[...,1], rgb[...,2]\n",
    "    return ((R / ((G+B)/2.0)) >= red_ratio).astype(np.uint8)\n",
    "\n",
    "def remove_small_components(mask_bin: np.ndarray, min_area: int) -> np.ndarray:\n",
    "    num, labels, stats, _ = cv2.connectedComponentsWithStats(mask_bin.astype(np.uint8), 8)\n",
    "    keep = np.zeros_like(mask_bin, dtype=np.uint8)\n",
    "    for i in range(1, num):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "            keep[labels == i] = 1\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c50865-28b4-4023-b339-8061992094c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[INFO] MedSAM weights loaded. Missing keys: 0, Unexpected keys: 0\n",
      "[READY] MedSAM loaded and mask generator initialized.\n"
     ]
    }
   ],
   "source": [
    "# MedSAM load cell: (1) optional Google Drive download → (2) load weights → (3) init mask generator\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys, re, subprocess\n",
    "import torch\n",
    "\n",
    "# 1) CONFIG: set your local destination and (optionally) paste your Google Drive share URL\n",
    "DEST = \"/home/philipdt/IKT-project/weights/medsam_vit_b.pth\"  # where to store/find the checkpoint\n",
    "\n",
    "# Segment-Anything backbone type for MedSAM\n",
    "SAM_TYPE = \"vit_b\"  # ViT-B checkpoint\n",
    "\n",
    "# Mask generator knobs (tweak as you like)\n",
    "POINTS_PER_SIDE = 24\n",
    "PRED_IOU_THRESH = 0.88\n",
    "STABILITY_THRESH = 0.90\n",
    "CROP_N_LAYERS = 0\n",
    "CROP_DOWNSCALE = 2\n",
    "MIN_MASK_REGION_AREA = 64\n",
    "\n",
    "# 2) Ensure segment-anything is installed and import\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "except ImportError:\n",
    "    print(\"[INFO] Installing segment-anything...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/facebookresearch/segment-anything.git\"])\n",
    "    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n",
    "\n",
    "ckpt_path = Path(DEST)\n",
    "\n",
    "# 4) Load MedSAM weights into the SAM ViT-B architecture\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Build the architecture first\n",
    "sam = sam_model_registry[SAM_TYPE](checkpoint=None)\n",
    "\n",
    "# Load weights (handle different dict formats)\n",
    "state = torch.load(str(ckpt_path), map_location=\"cpu\")\n",
    "if isinstance(state, dict) and \"state_dict\" in state:\n",
    "    state = state[\"state_dict\"]\n",
    "missing, unexpected = sam.load_state_dict(state, strict=False)\n",
    "print(f\"[INFO] MedSAM weights loaded. Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "sam.to(device=DEVICE)\n",
    "\n",
    "# 5) Create the automatic mask generator\n",
    "mask_gen = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=POINTS_PER_SIDE,\n",
    "    pred_iou_thresh=PRED_IOU_THRESH,\n",
    "    stability_score_thresh=STABILITY_THRESH,\n",
    "    crop_n_layers=CROP_N_LAYERS,\n",
    "    crop_n_points_downscale_factor=CROP_DOWNSCALE,\n",
    "    min_mask_region_area=MIN_MASK_REGION_AREA,\n",
    ")\n",
    "\n",
    "print(\"[READY] MedSAM loaded and mask generator initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26a2018-5cd2-4487-8a7e-7c09f7818890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys, time\n",
    "\n",
    "# def classify_masks(bgr: np.ndarray, masks: List[np.ndarray], class_set=\"extended\", debug=False) -> np.ndarray:\n",
    "#     H, W = bgr.shape[:2]\n",
    "#     labels = np.zeros((H, W), dtype=np.uint8)\n",
    "    \n",
    "#     # Precompute detection maps\n",
    "#     gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "#     vmap = vesselness(gray, sig_range=CFG.VESSEL_SIGMA_RANGE)\n",
    "#     spec = specular_mask(bgr, v_min=CFG.SPECULAR_V_MIN, s_max=CFG.SPECULAR_S_MAX)\n",
    "#     edges, _ = instrument_mask(bgr, edge_pct=CFG.INSTR_EDGE_PCT)\n",
    "#     edge_density = edges.astype(np.float32) / 255.0\n",
    "    \n",
    "#     # Improved tumor detection\n",
    "#     rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB).astype(np.float32) + 1e-6\n",
    "#     R, G, B = rgb[...,0], rgb[...,1], rgb[...,2]\n",
    "#     red_ratio_mask = ((R / ((G+B)/2.0)) >= CFG.TUMOR_RED_RATIO).astype(np.uint8)\n",
    "    \n",
    "#     # Use adaptive threshold for vesselness (try multiple percentiles)\n",
    "#     vessel_thresholds = [np.percentile(vmap, p) for p in [90, 95, 98]]\n",
    "    \n",
    "#     if debug:\n",
    "#         debug_classification(bgr, masks[:5])\n",
    "    \n",
    "#     # Sort masks by area (largest first) \n",
    "#     masks_with_area = [(m, m.sum()) for m in masks if m.sum() >= CFG.MIN_REGION_AREA]\n",
    "#     masks_with_area.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     covered = np.zeros((H, W), dtype=np.uint8)\n",
    "    \n",
    "#     for m, area in masks_with_area:\n",
    "#         # Remove overlap\n",
    "#         m_clean = (m & (~covered.astype(bool))).astype(np.uint8)\n",
    "#         if m_clean.sum() == 0:\n",
    "#             continue\n",
    "            \n",
    "#         m_bool = m_clean.astype(bool)\n",
    "        \n",
    "#         # Calculate ratios\n",
    "#         spec_ratio = float(spec[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "#         edge_ratio = float(edge_density[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "#         tumor_ratio = float(red_ratio_mask[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "        \n",
    "#         # Try multiple vessel thresholds\n",
    "#         vessel_ratios = [float((vmap[m_bool] > thr).mean()) for thr in vessel_thresholds]\n",
    "#         vessel_ratio = max(vessel_ratios)  # Use the most permissive\n",
    "        \n",
    "#         if class_set == \"basic\":\n",
    "#             cls_id = 1 if vessel_ratio > 0.05 else 2\n",
    "#         else:\n",
    "#             if spec_ratio > 0.02:       # CHANGED: more sensitive specular detection\n",
    "#                 cls_id = 5\n",
    "#             elif edge_ratio > 0.05:     # CHANGED: more sensitive instrument detection\n",
    "#                 cls_id = 4\n",
    "#             elif vessel_ratio > 0.06:   # CHANGED: more sensitive vessel detection\n",
    "#                 cls_id = 1\n",
    "#             elif tumor_ratio > 0.08:    # CHANGED: more sensitive tumor detection\n",
    "#                 cls_id = 3\n",
    "        \n",
    "#         labels[m_bool] = cls_id\n",
    "#         covered[m_bool] = 1\n",
    "        \n",
    "#         if covered.mean() >= CFG.COVERAGE_LIMIT:\n",
    "#             break\n",
    "            \n",
    "#     return labels\n",
    "\n",
    "def classify_masks(bgr: np.ndarray, masks: List[np.ndarray], class_set=\"extended\") -> np.ndarray:\n",
    "    H, W = bgr.shape[:2]\n",
    "    labels = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    # Precompute helper maps\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    vmap = vesselness(gray, sig_range=CFG.VESSEL_SIGMA_RANGE)\n",
    "    spec = specular_mask(bgr, v_min=CFG.SPECULAR_V_MIN, s_max=CFG.SPECULAR_S_MAX).astype(np.uint8)\n",
    "    edges, _ = instrument_mask(bgr, edge_pct=CFG.INSTR_EDGE_PCT)\n",
    "    edge_density = (edges.astype(np.float32) / 255.0)\n",
    "    \n",
    "    # Tumor detection\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB).astype(np.float32) + 1e-6\n",
    "    R, G, B = rgb[...,0], rgb[...,1], rgb[...,2]\n",
    "    red_ratio_mask = ((R / ((G+B)/2.0)) >= CFG.TUMOR_RED_RATIO).astype(np.uint8)\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "    a_star = lab[...,1]\n",
    "    a_thresh = np.percentile(a_star, 75)\n",
    "    red_lab_mask = (a_star >= a_thresh).astype(np.uint8)\n",
    "    tumor_like = np.maximum(red_ratio_mask, red_lab_mask)\n",
    "\n",
    "    # Global vesselness threshold\n",
    "    thr_vessel_img = np.percentile(vmap, CFG.VESSEL_PCT)\n",
    "    \n",
    "    print(f\"\\n=== DEBUG INFO ===\")\n",
    "    print(f\"Image size: {H}x{W}\")\n",
    "    print(f\"Number of masks: {len(masks)}\")\n",
    "    print(f\"Vesselness range: {vmap.min():.4f} to {vmap.max():.4f}\")\n",
    "    print(f\"Vesselness threshold ({CFG.VESSEL_PCT}th percentile): {thr_vessel_img:.4f}\")\n",
    "    print(f\"Specular pixels: {spec.sum()} ({100*spec.mean():.2f}%)\")\n",
    "    print(f\"High-edge pixels: {(edge_density > 0.05).sum()} ({100*(edge_density > 0.05).mean():.2f}%)\")\n",
    "    print(f\"Tumor-like pixels: {tumor_like.sum()} ({100*tumor_like.mean():.2f}%)\")\n",
    "    print(f\"Lab a* threshold: {a_thresh:.2f}\")\n",
    "\n",
    "    # Filter masks by size\n",
    "    valid_masks = [m for m in masks if m.sum() >= CFG.MIN_REGION_AREA]\n",
    "    print(f\"Valid masks (>= {CFG.MIN_REGION_AREA} pixels): {len(valid_masks)}\")\n",
    "    \n",
    "    if len(valid_masks) == 0:\n",
    "        print(\"No valid masks found!\")\n",
    "        return labels\n",
    "\n",
    "    # Priority scoring and sorting\n",
    "    mask_scores = []\n",
    "    for i, m in enumerate(valid_masks):\n",
    "        m = m.astype(np.uint8)\n",
    "        m_bool = m.astype(bool)\n",
    "        area = m.sum()\n",
    "        \n",
    "        spec_r = float(spec[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "        edge_r = float(edge_density[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "        vess_r = float((vmap[m_bool] > thr_vessel_img).mean()) if m_bool.sum() > 0 else 0\n",
    "        tum_r = float(tumor_like[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "        \n",
    "        priority = 3.0*spec_r + 2.0*edge_r + 1.5*vess_r + 1.0*tum_r\n",
    "        mask_scores.append((priority, m, i, area, spec_r, edge_r, vess_r, tum_r))\n",
    "\n",
    "    # Sort by priority then area\n",
    "    mask_scores.sort(key=lambda t: (t[0], t[3]), reverse=True)\n",
    "    covered = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    print(f\"\\nProcessing top {min(10, len(mask_scores))} masks:\")\n",
    "    assigned_classes = {}\n",
    "    \n",
    "    for priority, m, orig_idx, area, spec_r, edge_r, vess_r, tum_r in mask_scores[:10]:\n",
    "        # Remove overlap\n",
    "        m_clean = (m & (~covered.astype(bool))).astype(np.uint8)\n",
    "        remaining_area = m_clean.sum()\n",
    "        \n",
    "        if remaining_area == 0:\n",
    "            print(f\"Mask {orig_idx}: SKIPPED (fully covered)\")\n",
    "            continue\n",
    "\n",
    "        # Recalculate ratios for clean mask\n",
    "        m_bool = m_clean.astype(bool)\n",
    "        spec_ratio = float(spec[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "        edge_ratio = float(edge_density[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "        vessel_ratio = float((vmap[m_bool] > thr_vessel_img).mean()) if m_bool.sum() > 0 else 0\n",
    "        tumor_ratio = float(tumor_like[m_bool].mean()) if m_bool.sum() > 0 else 0\n",
    "\n",
    "        # Classify with very permissive thresholds for debugging\n",
    "        if class_set == \"basic\":\n",
    "            cls_id = 1 if vessel_ratio > 0.01 else 2  # Very low threshold\n",
    "        else:\n",
    "            # EXTREMELY permissive thresholds for debugging\n",
    "            if spec_ratio > 0.01:  # 1% specular pixels\n",
    "                cls_id = 5\n",
    "            elif edge_ratio > 0.02:  # 2% high-edge pixels  \n",
    "                cls_id = 4\n",
    "            elif vessel_ratio > 0.02:  # 2% vessel pixels\n",
    "                cls_id = 1\n",
    "            elif tumor_ratio > 0.03:  # 3% tumor-like pixels\n",
    "                cls_id = 3\n",
    "            else:\n",
    "                cls_id = 2\n",
    "\n",
    "        print(f\"Mask {orig_idx}: area={remaining_area}, spec={spec_ratio:.3f}, edge={edge_ratio:.3f}, \"\n",
    "              f\"vessel={vessel_ratio:.3f}, tumor={tumor_ratio:.3f} → class {cls_id}\")\n",
    "        \n",
    "        labels[m_bool] = cls_id\n",
    "        covered[m_bool] = 1\n",
    "        \n",
    "        assigned_classes[cls_id] = assigned_classes.get(cls_id, 0) + 1\n",
    "\n",
    "        if covered.mean() >= CFG.COVERAGE_LIMIT:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nFinal class distribution: {dict(sorted(assigned_classes.items()))}\")\n",
    "    print(f\"Unique classes in output: {np.unique(labels).tolist()}\")\n",
    "    return labels\n",
    "\n",
    "def generate_labels_for_dir(in_dir: Path, out_lbl: Path, out_vis: Path, class_set=\"basic\", max_vis=8):\n",
    "    ensure_dir(out_lbl); ensure_dir(out_vis)\n",
    "    paths = list_images(in_dir)\n",
    "    total = len(paths)\n",
    "\n",
    "    skipped = 0\n",
    "    fallbacks = 0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=total,\n",
    "        desc=in_dir.name,   # keep static\n",
    "        unit=\"img\",\n",
    "        ncols=80,\n",
    "        dynamic_ncols=False,\n",
    "        ascii=True,\n",
    "        leave=False,\n",
    "        file=sys.stderr     # same stream as default tqdm\n",
    "    )\n",
    "\n",
    "    for i, p in enumerate(paths, 1):\n",
    "        bgr = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            skipped += 1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        try:\n",
    "            anns = mask_gen.generate(rgb)\n",
    "        except IndexError as e:\n",
    "            if \"too many indices\" in str(e) or \"box_area\" in str(e):\n",
    "                from segment_anything import SamAutomaticMaskGenerator\n",
    "                mask_gen_fallback = SamAutomaticMaskGenerator(\n",
    "                    model=sam,\n",
    "                    points_per_side=CFG.POINTS_PER_SIDE,\n",
    "                    pred_iou_thresh=CFG.PRED_IOU_THRESH,\n",
    "                    stability_score_thresh=CFG.STABILITY_THRESH,\n",
    "                    crop_n_layers=0,\n",
    "                    crop_n_points_downscale_factor=CFG.CROP_DOWNSCALE,\n",
    "                    min_mask_region_area=CFG.MIN_MASK_REGION_AREA,\n",
    "                )\n",
    "                anns = mask_gen_fallback.generate(rgb)\n",
    "                fallbacks += 1\n",
    "            else:\n",
    "                pbar.close()\n",
    "                raise\n",
    "\n",
    "        masks = [a[\"segmentation\"].astype(np.uint8) for a in anns if a.get(\"area\", 0) >= CFG.MIN_MASK_REGION_AREA]\n",
    "        if not masks:\n",
    "            hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
    "            Hch, Sch, Vch = cv2.split(hsv)\n",
    "            tissue = ((Vch > 40) & (Sch > 20)).astype(np.uint8)\n",
    "            tissue = cv2.morphologyEx(tissue, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)))\n",
    "            labels = np.zeros(bgr.shape[:2], dtype=np.uint8)\n",
    "            labels[tissue.astype(bool)] = 2\n",
    "        else:\n",
    "            labels = classify_masks(bgr, masks, class_set=class_set)\n",
    "\n",
    "        cv2.imwrite(str(out_lbl / (p.stem + \".png\")), labels)\n",
    "        if i <= max_vis:\n",
    "            colors = CFG.CLASS_COLORS[class_set]\n",
    "            overlay = cv2.addWeighted(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), 1.0, colorize_mask(labels, colors), 0.45, 0.0)\n",
    "            cv2.imwrite(str(out_vis / (p.stem + \"_overlay.jpg\")), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        pbar.update(1)  # no set_postfix/desc updates here\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"[DONE] {in_dir.name}: {total} imgs | skipped={skipped}, fallbacks={fallbacks} → labels: {out_lbl} | vis: {out_vis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19bfbc6-4da2-4848-9f69-2b3775e9e136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HGC:   0%|                                             | 0/469 [00:00<?, ?img/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 647.00 MiB is free. Process 456913 has 3.42 GiB memory in use. Process 1906032 has 17.66 GiB memory in use. Process 2699547 has 8.38 GiB memory in use. Process 3830224 has 1.62 GiB memory in use. Of the allocated memory 1.21 GiB is allocated by PyTorch, and 48.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     out_lbl \u001b[38;5;241m=\u001b[39m out_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     out_vis \u001b[38;5;241m=\u001b[39m out_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mgenerate_labels_for_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_lbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_vis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_vis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ALL DONE] Labels saved to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, CFG\u001b[38;5;241m.\u001b[39mOUTPUT_LABEL_ROOT)\n",
      "Cell \u001b[0;32mIn[6], line 208\u001b[0m, in \u001b[0;36mgenerate_labels_for_dir\u001b[0;34m(in_dir, out_lbl, out_vis, class_set, max_vis)\u001b[0m\n\u001b[1;32m    206\u001b[0m rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(bgr, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     anns \u001b[38;5;241m=\u001b[39m \u001b[43mmask_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoo many indices\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox_area\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:236\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    234\u001b[0m cropped_im \u001b[38;5;241m=\u001b[39m image[y0:y1, x0:x1, :]\n\u001b[1;32m    235\u001b[0m cropped_im_size \u001b[38;5;241m=\u001b[39m cropped_im\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_im\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Get points for this crop\u001b[39;00m\n\u001b[1;32m    239\u001b[0m points_scale \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(cropped_im_size)[\u001b[38;5;28;01mNone\u001b[39;00m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(input_image, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m input_image_torch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_image_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:358\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    354\u001b[0m rel_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,hkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rh)\n\u001b[1;32m    355\u001b[0m rel_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,wkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rw)\n\u001b[1;32m    357\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 358\u001b[0m     \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_h\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m rel_w[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    359\u001b[0m )\u001b[38;5;241m.\u001b[39mview(B, q_h \u001b[38;5;241m*\u001b[39m q_w, k_h \u001b[38;5;241m*\u001b[39m k_w)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 647.00 MiB is free. Process 456913 has 3.42 GiB memory in use. Process 1906032 has 17.66 GiB memory in use. Process 2699547 has 8.38 GiB memory in use. Process 3830224 has 1.62 GiB memory in use. Of the allocated memory 1.21 GiB is allocated by PyTorch, and 48.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "in_root = Path(CFG.ORIG_ROOT)\n",
    "out_root = Path(CFG.OUTPUT_LABEL_ROOT)\n",
    "class_set = CFG.CLASS_SET\n",
    "\n",
    "for cls in CFG.CLASSES:  # no outer tqdm\n",
    "    src_dir = in_root / cls\n",
    "    if not src_dir.is_dir():\n",
    "        # Avoid printing inside the progress run; skip quietly or log after\n",
    "        continue\n",
    "    out_lbl = out_root / cls / \"labels\"\n",
    "    out_vis = out_root / cls / \"vis\"\n",
    "    generate_labels_for_dir(src_dir, out_lbl, out_vis, class_set=class_set, max_vis=8)\n",
    "print(\"[ALL DONE] Labels saved to:\", CFG.OUTPUT_LABEL_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aca8e2-a09e-452a-a433-344a563b084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, matplotlib.pyplot as plt, numpy as np, cv2\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _colorize_mask(mask, colors):\n",
    "    out = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for k, c in colors.items():\n",
    "        out[mask == k] = c\n",
    "    return out\n",
    "\n",
    "def show_examples(cls=\"HGC\", n=6):\n",
    "    vis_dir = Path(CFG.OUTPUT_LABEL_ROOT) / cls / \"vis\"\n",
    "    lbl_dir = Path(CFG.OUTPUT_LABEL_ROOT) / cls / \"labels\"\n",
    "    img_dir = Path(CFG.ORIG_ROOT) / cls\n",
    "\n",
    "    if not vis_dir.is_dir():\n",
    "        print(\"No vis folder yet:\", vis_dir); return\n",
    "    files = sorted([p for p in vis_dir.iterdir() if p.suffix.lower() in [\".jpg\",\".png\"]])\n",
    "    if not files:\n",
    "        print(\"No images in\", vis_dir); return\n",
    "\n",
    "    picks = random.sample(files, k=min(n, len(files)))\n",
    "    rows = len(picks)   # one row per sample\n",
    "    cols = 3            # Original | Label | Vis\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(14, 3.8*rows))\n",
    "    if rows == 1:\n",
    "        axs = np.array([axs])  # make it 2D for uniform indexing\n",
    "\n",
    "    # Choose color map from config\n",
    "    colors = CFG.CLASS_COLORS[CFG.CLASS_SET] if hasattr(CFG, \"CLASS_SET\") else CFG.CLASS_COLORS\n",
    "    IMG_EXTS = [\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\",\".PNG\",\".JPG\"]\n",
    "\n",
    "    for row_idx, vis_path in enumerate(picks):\n",
    "        base_stem = vis_path.stem[:-8] if vis_path.stem.endswith(\"_overlay\") else vis_path.stem\n",
    "\n",
    "        # Find original by stem\n",
    "        orig_path = None\n",
    "        for ext in IMG_EXTS:\n",
    "            cand = img_dir / f\"{base_stem}{ext}\"\n",
    "            if cand.exists():\n",
    "                orig_path = cand; break\n",
    "\n",
    "        # Load original\n",
    "        orig_rgb = None\n",
    "        if orig_path is not None:\n",
    "            bgr = cv2.imread(str(orig_path), cv2.IMREAD_COLOR)\n",
    "            if bgr is not None:\n",
    "                orig_rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load label (indexed PNG)\n",
    "        lbl_path = lbl_dir / f\"{base_stem}.png\"\n",
    "        lbl = cv2.imread(str(lbl_path), cv2.IMREAD_UNCHANGED) if lbl_path.exists() else None\n",
    "        lbl_ids = np.unique(lbl) if isinstance(lbl, np.ndarray) else np.array([])\n",
    "\n",
    "        # Load existing vis overlay\n",
    "        vis_bgr = cv2.imread(str(vis_path))\n",
    "        vis_rgb = cv2.cvtColor(vis_bgr, cv2.COLOR_BGR2RGB) if vis_bgr is not None else None\n",
    "\n",
    "        # Original\n",
    "        ax = axs[row_idx, 0]\n",
    "        if orig_rgb is not None:\n",
    "            ax.imshow(orig_rgb); ax.set_title(f\"Original\\n{orig_path.name}\", fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Original not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Colorized Label\n",
    "        ax = axs[row_idx, 1]\n",
    "        if isinstance(lbl, np.ndarray):\n",
    "            col = _colorize_mask(lbl, colors)\n",
    "            ax.imshow(col); ax.set_title(f\"Label IDs: {lbl_ids.tolist()}\", fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Label not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Vis overlay\n",
    "        ax = axs[row_idx, 2]\n",
    "        if vis_rgb is not None:\n",
    "            ax.imshow(vis_rgb); ax.set_title(vis_path.name, fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Vis not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_examples(\"HGC\", n=6)\n",
    "show_examples(\"LGC\", n=6)\n",
    "show_examples(\"NST\", n=6)\n",
    "show_examples(\"NTL\", n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a6c25-c76d-4e0f-9330-c55a0c720f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, matplotlib.pyplot as plt, numpy as np, cv2\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "IMG_EXTS = [\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\",\".PNG\",\".JPG\"]\n",
    "\n",
    "def _find_original(img_dir: Path, stem: str):\n",
    "    for ext in IMG_EXTS:\n",
    "        cand = img_dir / f\"{stem}{ext}\"\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "def _colorize_mask(mask, colors):\n",
    "    out = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for k, c in colors.items():\n",
    "        out[mask == k] = c\n",
    "    return out\n",
    "\n",
    "def dataset_class_summary(cls=\"HGC\"):\n",
    "    lbl_dir = Path(CFG.OUTPUT_LABEL_ROOT) / cls / \"labels\"\n",
    "    if not lbl_dir.is_dir():\n",
    "        print(\"No labels folder:\", lbl_dir); return\n",
    "    counts = Counter()\n",
    "    file_counts = Counter()\n",
    "    for lp in sorted(lbl_dir.glob(\"*.png\")):\n",
    "        lbl = cv2.imread(str(lp), cv2.IMREAD_UNCHANGED)\n",
    "        if lbl is None: \n",
    "            continue\n",
    "        ids = set(np.unique(lbl).tolist())\n",
    "        for cid in ids:\n",
    "            counts[cid] += int((lbl == cid).sum())\n",
    "        for cid in ids:\n",
    "            file_counts[cid] += 1\n",
    "    print(f\"[SUMMARY] Files with class (by presence): {dict(sorted(file_counts.items()))}\")\n",
    "    print(f\"[SUMMARY] Pixel counts by class: {dict(sorted(counts.items()))}\")\n",
    "\n",
    "def show_examples_diverse(cls=\"HGC\", n=6, prefer_ids=(1,3,4,5,2), seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    lbl_dir = Path(CFG.OUTPUT_LABEL_ROOT) / cls / \"labels\"\n",
    "    vis_dir = Path(CFG.OUTPUT_LABEL_ROOT) / cls / \"vis\"\n",
    "    img_dir = Path(CFG.ORIG_ROOT) / cls\n",
    "\n",
    "    if not lbl_dir.is_dir():\n",
    "        print(\"No labels folder:\", lbl_dir); return\n",
    "\n",
    "    # 1) Index labels -> which classes each image contains (exclude 0 from selection)\n",
    "    idx = []  # list of (stem, present_ids_set_without_bg)\n",
    "    present_ids_overall = set()\n",
    "    for lp in sorted(lbl_dir.glob(\"*.png\")):\n",
    "        stem = lp.stem\n",
    "        lbl = cv2.imread(str(lp), cv2.IMREAD_UNCHANGED)\n",
    "        if lbl is None:\n",
    "            continue\n",
    "        ids = set(np.unique(lbl).tolist())\n",
    "        ids_nobg = {cid for cid in ids if cid != 0}\n",
    "        if ids_nobg:\n",
    "            present_ids_overall |= ids_nobg\n",
    "        idx.append((stem, ids_nobg))\n",
    "\n",
    "    if not idx:\n",
    "        print(\"No readable label PNGs found in:\", lbl_dir)\n",
    "        return\n",
    "\n",
    "    # 2) Build candidates per class\n",
    "    by_class = defaultdict(list)\n",
    "    for stem, s in idx:\n",
    "        for cid in s:\n",
    "            by_class[cid].append(stem)\n",
    "\n",
    "    # 3) Greedy selection to maximize diversity: pick one per preferred class if available\n",
    "    chosen = []\n",
    "    chosen_set = set()\n",
    "\n",
    "    # Ensure we only request classes that actually exist\n",
    "    effective_prefer = [cid for cid in prefer_ids if cid in present_ids_overall]\n",
    "\n",
    "    for cid in effective_prefer:\n",
    "        candidates = [st for st in by_class[cid] if st not in chosen_set]\n",
    "        if candidates:\n",
    "            pick = random.choice(candidates)\n",
    "            chosen.append(pick)\n",
    "            chosen_set.add(pick)\n",
    "            if len(chosen) >= n:\n",
    "                break\n",
    "\n",
    "    # 4) If still need more, fill with images that add the most new preferred classes\n",
    "    def score(stem):\n",
    "        s = next((s for st, s in idx if st == stem), set())\n",
    "        return len({cid for cid in s if cid in effective_prefer})\n",
    "    if len(chosen) < n:\n",
    "        remaining = [st for st, s in idx if st not in chosen_set]\n",
    "        remaining.sort(key=score, reverse=True)\n",
    "        for st in remaining:\n",
    "            if len(chosen) >= n:\n",
    "                break\n",
    "            chosen.append(st); chosen_set.add(st)\n",
    "\n",
    "    # 5) Plot: Original | Colorized Label | Vis overlay (compute overlay if missing)\n",
    "    rows, cols = len(chosen), 3\n",
    "    colors = CFG.CLASS_COLORS[CFG.CLASS_SET] if hasattr(CFG, \"CLASS_SET\") else CFG.CLASS_COLORS\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(14, 3.8*rows))\n",
    "    if rows == 1:\n",
    "        axs = np.array([axs])\n",
    "\n",
    "    for row_idx, stem in enumerate(chosen):\n",
    "        lbl_path = lbl_dir / f\"{stem}.png\"\n",
    "        vis_path = vis_dir / f\"{stem}_overlay.jpg\"\n",
    "        orig_path = _find_original(img_dir, stem)\n",
    "\n",
    "        # Load label\n",
    "        lbl = cv2.imread(str(lbl_path), cv2.IMREAD_UNCHANGED)\n",
    "        lbl_ids = np.unique(lbl) if isinstance(lbl, np.ndarray) else np.array([])\n",
    "\n",
    "        # Load/create display images\n",
    "        # Original\n",
    "        orig_rgb = None\n",
    "        if orig_path is not None:\n",
    "            bgr = cv2.imread(str(orig_path), cv2.IMREAD_COLOR)\n",
    "            if bgr is not None:\n",
    "                orig_rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Colorized label\n",
    "        col = _colorize_mask(lbl, colors) if isinstance(lbl, np.ndarray) else None\n",
    "\n",
    "        # Vis overlay: use saved one if exists, else build on-the-fly\n",
    "        vis_rgb = None\n",
    "        if vis_path.exists():\n",
    "            vis_bgr = cv2.imread(str(vis_path))\n",
    "            if vis_bgr is not None:\n",
    "                vis_rgb = cv2.cvtColor(vis_bgr, cv2.COLOR_BGR2RGB)\n",
    "        elif orig_rgb is not None and col is not None:\n",
    "            vis_rgb = cv2.addWeighted(orig_rgb, 1.0, col, 0.45, 0.0)\n",
    "\n",
    "        # Plot\n",
    "        ax = axs[row_idx, 0]\n",
    "        if orig_rgb is not None:\n",
    "            ax.imshow(orig_rgb); ax.set_title(f\"Original\\n{orig_path.name}\", fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Original not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        ax = axs[row_idx, 1]\n",
    "        if col is not None:\n",
    "            ax.imshow(col); ax.set_title(f\"Label IDs: {lbl_ids.tolist()}\", fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Label not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        ax = axs[row_idx, 2]\n",
    "        if vis_rgb is not None:\n",
    "            ax.imshow(vis_rgb); ax.set_title(f\"{stem}_overlay.jpg\", fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Vis overlay not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "# Show up to 6 examples prioritizing vessel(1), tumor(3), instrument(4), specular(5), then mucosa(2).\n",
    "# If some classes aren't present in HGC, it will skip them and still fill the grid.\n",
    "dataset_class_summary(\"HGC\")\n",
    "show_examples_diverse(\"HGC\", n=6, prefer_ids=(1,3,4,5,2), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7bc4a-879d-4cae-a5fe-1e9feecbbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np, cv2, os\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Originals and MedSAM outputs\n",
    "ORIG_ROOT = Path(\"/home/philipdt/IKT-project/data/EndoscopicBladderTissue\")  # adjust if needed\n",
    "SEG_ROOT  = Path(\"/home/philipdt/IKT-project/auto_labels_medsam\")\n",
    "\n",
    "# Classes and label IDs (update if your mapping differs)\n",
    "CLASSES = [\"HGC\", \"LGC\", \"NST\", \"NTL\"]\n",
    "# 0:bg, 1:vessel, 2:mucosa, 3:tumor, 4:instrument, 5:specular\n",
    "KEEP_SETS = {\n",
    "    \"masked_all\":        {1,2,3,4,5},\n",
    "    \"masked_tumor_only\": {3},\n",
    "}\n",
    "GEN_OVERLAY_ALL = True  # set False if you don’t want full overlays for every image\n",
    "\n",
    "# Overlay palette (BGR)\n",
    "PALETTE = {\n",
    "    1: (255, 64, 64),    # vessel - blue-ish (OpenCV uses BGR)\n",
    "    2: (64, 255, 64),    # mucosa - green\n",
    "    3: (64, 64, 255),    # tumor - red-ish\n",
    "    4: (255, 255, 64),   # instrument - cyan-ish\n",
    "    5: (255, 64, 255),   # specular - magenta\n",
    "}\n",
    "OVERLAY_ALPHA = 0.45  # blending factor (0..1)\n",
    "IMG_EXTS = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\",\".PNG\",\".JPG\",\".JPEG\",\".BMP\",\".TIF\",\".TIFF\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7176ece-4fb2-4351-b48f-ffdca1d3037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orig_by_stem(root: Path, cls: str, stem: str):\n",
    "    d = root / cls\n",
    "    for e in IMG_EXTS:\n",
    "        p = d / f\"{stem}{e}\"\n",
    "        if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "def load_label(path: Path):\n",
    "    # Expect indexed PNG (uint8/16)\n",
    "    lbl = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    if lbl is None:\n",
    "        raise FileNotFoundError(f\"Could not read label: {path}\")\n",
    "    return lbl\n",
    "\n",
    "def ensure_label_size(lbl, w, h):\n",
    "    if lbl.shape[1] != w or lbl.shape[0] != h:\n",
    "        lbl = cv2.resize(lbl, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    return lbl\n",
    "\n",
    "def apply_mask(img_rgb: np.ndarray, lbl: np.ndarray, keep_ids: set):\n",
    "    mask = np.isin(lbl, list(keep_ids)).astype(np.uint8)\n",
    "    out = img_rgb.copy()\n",
    "    out[mask == 0] = 0\n",
    "    return out\n",
    "\n",
    "def make_overlay(img_rgb: np.ndarray, lbl: np.ndarray, alpha=0.45):\n",
    "    overlay = img_rgb.copy()\n",
    "    base = img_rgb.copy()\n",
    "    for cls_id, color_bgr in PALETTE.items():\n",
    "        if cls_id == 0: continue\n",
    "        m = (lbl == cls_id)\n",
    "        if not np.any(m): continue\n",
    "        overlay[m] = color_bgr\n",
    "    blended = cv2.addWeighted(overlay, alpha, base, 1-alpha, 0)\n",
    "    return blended[..., ::-1]  # convert BGR->RGB appearance if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f0ad6-2f69-4343-a6a1-f23b3eeb3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_masked_variants():\n",
    "    total = 0\n",
    "    made = {k: 0 for k in KEEP_SETS.keys()}\n",
    "    ovl_made = 0\n",
    "\n",
    "    for cls in CLASSES:\n",
    "        lbl_dir = SEG_ROOT / cls / \"labels\"\n",
    "        if not lbl_dir.is_dir():\n",
    "            print(f\"[WARN] Missing labels dir: {lbl_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare out dirs\n",
    "        out_dirs = {name: SEG_ROOT/cls/name for name in KEEP_SETS.keys()}\n",
    "        for d in out_dirs.values(): d.mkdir(parents=True, exist_ok=True)\n",
    "        ovl_dir = SEG_ROOT/cls/\"overlay_all\"\n",
    "        if GEN_OVERLAY_ALL: ovl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        label_paths = sorted(lbl_dir.glob(\"*.png\"))\n",
    "        pbar = tqdm(label_paths, desc=f\"[{cls}] exporting\", leave=False)\n",
    "        for lp in pbar:\n",
    "            stem = lp.stem\n",
    "            # locate original\n",
    "            op = find_orig_by_stem(ORIG_ROOT, cls, stem)\n",
    "            if op is None:\n",
    "                pbar.set_postfix_str(\"orig-missing\")\n",
    "                continue\n",
    "\n",
    "            # load original + label\n",
    "            img = Image.open(op).convert(\"RGB\")\n",
    "            img_np = np.array(img)  # HxWx3 RGB\n",
    "            h, w = img_np.shape[:2]\n",
    "\n",
    "            lbl = load_label(lp)\n",
    "            lbl = ensure_label_size(lbl, w, h)\n",
    "\n",
    "            # masked variants\n",
    "            for name, keep_ids in KEEP_SETS.items():\n",
    "                out = apply_mask(img_np, lbl, keep_ids)\n",
    "                Image.fromarray(out).save(out_dirs[name]/f\"{stem}.png\")\n",
    "                made[name] += 1\n",
    "\n",
    "            # overlay (optional)\n",
    "            if GEN_OVERLAY_ALL:\n",
    "                ov = make_overlay(img_np[..., ::-1], lbl, alpha=OVERLAY_ALPHA)  # ensure proper color mapping\n",
    "                Image.fromarray(ov).save(ovl_dir/f\"{stem}.png\")\n",
    "                ovl_made += 1\n",
    "\n",
    "            total += 1\n",
    "\n",
    "    print(f\"Processed labels: {total}\")\n",
    "    for k,v in made.items(): print(f\"  {k}: {v} images\")\n",
    "    if GEN_OVERLAY_ALL: print(f\"  overlay_all: {ovl_made} images\")\n",
    "\n",
    "export_masked_variants()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
